{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports \n",
    "Necessary imports, based in part on [this](https://www.kaggle.com/code/awsaf49/planttraits2024-kerascv-starter-notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\n",
    "\n",
    "# deep learning\n",
    "import keras_cv\n",
    "import keras\n",
    "from keras import ops\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import umap.umap_ as umap\n",
    "\n",
    "# data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import joblib\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "\n",
    "nn = model.Model(\"model1\")\n",
    "\n",
    "print(nn.name) # model1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading\n",
    "The general format of each instance of the data is \n",
    "```\n",
    "[ids*, ancillary data*, trait means*, traits sd*, image path]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import config\n",
    "DATA_PATH = \"../data\"\n",
    "\n",
    "# creating df that has image path and related ancillary data\n",
    "df = pd.read_csv(DATA_PATH + \"/train.csv\")\n",
    "df['image_path'] = f'{DATA_PATH}/train_images/' + str(df['id']) + '.jpeg'\n",
    "df.loc[:, config.aux_class_names] = df.loc[:, config.aux_class_names].fillna(-1)\n",
    "display(df.head(2))\n",
    "print(df.shape[0]) \n",
    "\n",
    "# same df but for the test data\n",
    "test_df = pd.read_csv(DATA_PATH + \"/test.csv\")\n",
    "df['image_path'] = f'{DATA_PATH}/test_images'+ str(test_df['id']) + '.jpeg'\n",
    "FEATURE_COLS = list(test_df.columns[1:-1])\n",
    "display(test_df.head(2))\n",
    "print(test_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## traits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mean traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_traits = df.iloc[:, 164:170]\n",
    "sd_traits = df.iloc[:, 170:176]\n",
    "display(mean_traits.head(2))\n",
    "\n",
    "\n",
    "def mean_normalizer(col):\n",
    "    mean = mean_traits.iloc[:, col]\n",
    "    mean = mean.drop_duplicates()\n",
    "    return (mean - mean.min()) / (mean.max() - mean.min())\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(6): \n",
    "    plt.hist(mean_normalizer(i), log=True, bins=100, alpha=0.5, label=f'Normalized x{i} mean')\n",
    "    \n",
    "plt.title('Normalized mean trait distributions')\n",
    "plt.xlabel('Normalized mean trait value')\n",
    "plt.ylabel('Log count')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_11_mean = mean_traits.iloc[:, 1]\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# remove outliers\n",
    "x_11_mean = x_11_mean[x_11_mean.between(x_11_mean.quantile(0), x_11_mean.quantile(1))]\n",
    "\n",
    "x_11_mean = x_11_mean.drop_duplicates()\n",
    "\n",
    "ax[0].hist(x_11_mean, bins=100, log=True, alpha=0.5)\n",
    "ax[0].set_title('x11 mean distribution')\n",
    "ax[0].set_xlabel('x11 mean value')\n",
    "ax[0].set_ylabel('Log count')\n",
    "\n",
    "x_11_mean = x_11_mean[x_11_mean.between(x_11_mean.quantile(0), x_11_mean.quantile(0.99))]\n",
    "print(x_11_mean.count())\n",
    "ax[1].hist(x_11_mean, bins=100, alpha=0.5)\n",
    "ax[1].set_title('x11 mean distribution (99th percentile)')\n",
    "ax[1].set_xlabel('x11 mean value')\n",
    "ax[1].set_ylabel('Instance count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 6, figsize=(30, 5))\n",
    "\n",
    "for i in range(6):\n",
    "    x_mean = mean_traits.iloc[:, i]\n",
    "    x_mean = x_mean[x_mean.between(x_mean.quantile(0), x_mean.quantile(0.99))]\n",
    "    x_mean = x_mean.drop_duplicates()\n",
    "    \n",
    "    ax[i].hist(x_mean, bins=100, log=True, alpha=0.5, label=f'Normalized x{i} mean')\n",
    "    ax[i].set_title(f'x{i} mean distribution (99th percentile)')\n",
    "    ax[i].set_xlabel(f'x{i} mean value')\n",
    "    ax[i].set_ylabel('Log count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us some idea of the distribution of the targets we are training for. One thing to consider is that this distribution of traits might not be representative of the general traits of plant species. \n",
    "*Notes*\n",
    "- we can maybe check some location data to see where most of the images came from to see if certain traits indicate a geographic bias "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sd traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_traits = df.iloc[:, 170:176]\n",
    "display(sd_traits.head(2))\n",
    "def sd_normalizer(col):\n",
    "    sd = sd_traits.iloc[:, col]\n",
    "    return (sd - sd.min()) / (sd.max() - sd.min())\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(6):  \n",
    "    plt.hist(sd_normalizer(i), log=True, bins=100, alpha=0.5, label=f'Normalized SD x{i}')\n",
    "\n",
    "plt.title('Normalized standard deviation trait distributions')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might be an even better representative that we are training towards a biased sample. Intuitively I'd assume that traits would probably have a large variance if we have a geographic diversity in our sample, but here it seems this is less so the case for most of the deviations maybe ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ancillary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancillary_data = df.iloc[:, 1:164]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "# worldclim \n",
    "worldclim = ancillary_data.filter(like='WORLDCLIM')\n",
    "\n",
    "g = sns.pairplot(worldclim, diag_kind='kde', plot_kws={'alpha': 0.5})\n",
    "\n",
    "# remove labels \n",
    "for ax in g.axes.flatten():\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xlabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worldclim = ancillary_data.filter(like='WORLDCLIM')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "corr = worldclim.corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', cbar=True)\n",
    "ax.set_title('Worldclim feature correlation heatmap')\n",
    "\n",
    "high_corr = [i for i in corr[corr > 0.9].stack().index.tolist() if i[0] != i[1]]\n",
    "high_corr = list(set([tuple(sorted(i)) for i in high_corr]))\n",
    "\n",
    "scaled_worldclim_data = (worldclim - worldclim.min()) / (worldclim.max() - worldclim.min())\n",
    "print(high_corr)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "pca_result = pca.fit_transform(scaled_worldclim_data)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Plot PCA\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.scatter(pca_result[:, 0], pca_result[:, 1], pca_result[:, 2], c=pca_result[:, 0], cmap='viridis', s=10)\n",
    "ax1.set_title('PCA of worldclim features')\n",
    "\n",
    "reconstructed_pca = pca.inverse_transform(pca_result)\n",
    "loss_pca = np.mean((scaled_worldclim_data - reconstructed_pca) ** 2)\n",
    "\n",
    "print(f'PCA Reconstruction Loss: {loss_pca}')\n",
    "\n",
    "# UMAP\n",
    "reducer = umap.UMAP(n_components=3)\n",
    "embedding = reducer.fit_transform(scaled_worldclim_data)\n",
    "\n",
    "# Plot UMAP\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "ax2.scatter(embedding[:, 0], embedding[:, 1], embedding[:, 2], c=embedding[:, 0], cmap='viridis', s=10)\n",
    "ax2.set_title('UMAP embedding of worldclim features')\n",
    "\n",
    "reconstructed_umap = reducer.inverse_transform(embedding)\n",
    "loss_umap = np.mean((scaled_worldclim_data - reconstructed_umap) ** 2)\n",
    "\n",
    "print(f'UMAP Reconstruction Loss: {loss_umap}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we start of by just looking at worldclim subset of our training data and testing out some of the basic methods we are going to then use on the broader dataset, namely finding correlations and then comparing the performance of PCA vs UAMP in the dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancillary_data = df.iloc[:, 1:164]\n",
    " \n",
    "# select allcolumns with a name containing WORLDCLIM\n",
    "worldclim   = ancillary_data.filter(like='WORLDCLIM')\n",
    "soil        = ancillary_data.filter(like='SOIL')\n",
    "modis       = ancillary_data.filter(like='MODIS')\n",
    "vod         = ancillary_data.filter(like='VOD')\n",
    "\n",
    "print(\n",
    "    f\"lengths {len(worldclim.columns)}, {len(soil.columns)}, {len(modis.columns)}, {len(vod.columns)}\"\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 x 1 subplot of correlation heatmaps \n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# soil \n",
    "\n",
    "soil_corr = soil.corr()\n",
    "sns.heatmap(soil_corr, ax=ax[0], cmap='coolwarm', annot=False)\n",
    "\n",
    "ax[0].set_title('Soil feature correlation heatmap')\n",
    "ax[0].set_xticklabels([])\n",
    "ax[0].set_yticklabels([])\n",
    "\n",
    "# modis\n",
    "\n",
    "modis_corr = modis.corr()\n",
    "sns.heatmap(modis_corr, ax=ax[1], cmap='coolwarm', annot=False)\n",
    "\n",
    "ax[1].set_title('MODIS feature correlation heatmap')\n",
    "ax[1].set_xticklabels([])\n",
    "ax[1].set_yticklabels([])\n",
    "\n",
    "# vod\n",
    "\n",
    "vod_corr = vod.corr()\n",
    "sns.heatmap(vod_corr, ax=ax[2], cmap='coolwarm', annot=False)\n",
    "\n",
    "ax[2].set_title('VOD feature correlation heatmap')\n",
    "ax[2].set_xticklabels([])\n",
    "ax[2].set_yticklabels([])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = [worldclim.corr(), soil.corr(), modis.corr(), vod.corr()]\n",
    "\n",
    "high_corr = [] \n",
    "\n",
    "for corr in correlations:\n",
    "    high_corr += [\n",
    "        i for i in corr[corr > 0.9].stack().index.tolist() if i[0] != i[1]\n",
    "    ]\n",
    "    \n",
    "mod_corr = []\n",
    "\n",
    "for corr in correlations:\n",
    "    mod_corr += [\n",
    "        i for i in corr[corr > 0.65].stack().index.tolist() if i[0] != i[1]\n",
    "    ]\n",
    "    \n",
    "total_cols = 164 \n",
    "all_comb_num = total_cols * (total_cols - 1) / 2\n",
    "    \n",
    "high_corr = list(set([tuple(sorted(i)) for i in high_corr]))\n",
    "print(\n",
    "    f'num combinations with high correlation: {len(high_corr)}' + \n",
    "    f' ({len(high_corr) / all_comb_num * 100:.2f}%)' +\n",
    "    f' tot = {all_comb_num}' \n",
    ")\n",
    "print(\n",
    "    f'num combinations with moderate correlation: {len(mod_corr)}' +\n",
    "    f' ({len(mod_corr) / all_comb_num * 100:.2f}%)' +\n",
    "    f' tot = {all_comb_num}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "\n",
    "ancillary_data = df.iloc[:, 1:164]\n",
    "\n",
    "rand_col_range = np.random.choice(ancillary_data.shape[1], 20)\n",
    "\n",
    "# random sample of ancillary data subset \n",
    "ancillary_data = ancillary_data.iloc[:, rand_col_range]\n",
    "\n",
    "# PCA \n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "ancillary_data = scaler.fit_transform(ancillary_data)\n",
    "\n",
    "pca_result = pca.fit_transform(ancillary_data)\n",
    "\n",
    "pca_loss = np.mean((ancillary_data - pca.inverse_transform(pca_result)) ** 2)\n",
    "\n",
    "# UMAP\n",
    "reducer = umap.UMAP(n_components=30)\n",
    "\n",
    "embedding = reducer.fit_transform(ancillary_data)\n",
    "\n",
    "umap_loss = np.mean((ancillary_data - reducer.inverse_transform(embedding)) ** 2)\n",
    "\n",
    "print(\n",
    "    f'PCA Reconstruction Loss: {pca_loss:.5f}\\n' +\n",
    "    f'UMAP Reconstruction Loss: {umap_loss:.5f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter over all rows in df and look for missing data\n",
    "\n",
    "bad_rows = []\n",
    "class_names = [n for n in config.aux_class_names if n.startswith(\"X\") and n.endswith(\"_sd\")]  # there is some missing data in the X*_sd columns\n",
    "for i, row in df.iterrows():\n",
    "    # check if any of those is a 0, -1 or 1\n",
    "    if row[class_names].isin([0, -1, 1]).any():\n",
    "        bad_rows.append(i)\n",
    "\n",
    "# remove duplicates\n",
    "bad_rows = set(bad_rows)\n",
    "percentage_bad = len(bad_rows) / len(df) * 100\n",
    "print(f\"Found {len(bad_rows)} bad rows ({len(bad_rows)}/{len(df)}) ({percentage_bad:.2f}%)\")\n",
    "\n",
    "# display top 5 bad rows\n",
    "df.iloc[list(bad_rows)].tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### where ?\n",
    "\n",
    "Features marked as `X*_sd` are missing for almost 30% of the data (16387/55489) (29.53%). The rest of the data seems to be complete, with no other missing/zeroed values. We can ignore these rows, or we can simply ignore those features as it is a significant amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data split"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
