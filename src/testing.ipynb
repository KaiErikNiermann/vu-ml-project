{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports \n",
    "Necessary imports, based in part on [this](https://www.kaggle.com/code/awsaf49/planttraits2024-kerascv-starter-notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\n",
    "\n",
    "# deep learning\n",
    "import keras_cv\n",
    "import keras\n",
    "from keras import ops\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import umap.umap_ as umap\n",
    "\n",
    "# data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import joblib\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "\n",
    "nn = model.Model(\"model1\")\n",
    "\n",
    "print(nn.name) # model1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading\n",
    "The general format of each instance of the data is \n",
    "```\n",
    "[ids*, ancillary data*, trait means*, traits sd*, image path]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import config\n",
    "DATA_PATH = \"../data\"\n",
    "\n",
    "# creating df that has image path and related ancillary data\n",
    "df = pd.read_csv(DATA_PATH + \"/train.csv\")\n",
    "df['image_path'] = f'{DATA_PATH}/train_images/' + df['id'].astype(str) + '.jpeg'\n",
    "df.loc[:, config.aux_class_names] = df.loc[:, config.aux_class_names].fillna(-1)\n",
    "display(df.head(2))\n",
    "print(df.shape[0]) \n",
    "\n",
    "# same df but for the test data\n",
    "test_df = pd.read_csv(DATA_PATH + \"/test.csv\")\n",
    "test_df['image_path'] = f'{DATA_PATH}/test_images'+ test_df['id'].astype(str) + '.jpeg'\n",
    "FEATURE_COLS = list(test_df.columns[1:-1])\n",
    "display(test_df.head(2))\n",
    "print(test_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## traits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mean traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_traits = df.iloc[:, 164:170]\n",
    "sd_traits = df.iloc[:, 170:176]\n",
    "display(mean_traits.head(2))\n",
    "\n",
    "\n",
    "def mean_normalizer(col):\n",
    "    mean = mean_traits.iloc[:, col]\n",
    "    mean = mean.drop_duplicates()\n",
    "    return (mean - mean.min()) / (mean.max() - mean.min())\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(6): \n",
    "    plt.hist(mean_normalizer(i), log=True, bins=100, alpha=0.5, label=f'Normalized x{i} mean')\n",
    "    \n",
    "plt.title('Normalized mean trait distributions')\n",
    "plt.xlabel('Normalized mean trait value')\n",
    "plt.ylabel('Log count')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_11_mean = mean_traits.iloc[:, 1]\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# remove outliers\n",
    "x_11_mean = x_11_mean[x_11_mean.between(x_11_mean.quantile(0), x_11_mean.quantile(1))]\n",
    "\n",
    "x_11_mean = x_11_mean.drop_duplicates()\n",
    "\n",
    "ax[0].hist(x_11_mean, bins=100, log=True, alpha=0.5)\n",
    "ax[0].set_title('x11 mean distribution')\n",
    "ax[0].set_xlabel('x11 mean value')\n",
    "ax[0].set_ylabel('Log count')\n",
    "\n",
    "x_11_mean = x_11_mean[x_11_mean.between(x_11_mean.quantile(0), x_11_mean.quantile(0.99))]\n",
    "print(x_11_mean.count())\n",
    "ax[1].hist(x_11_mean, bins=100, alpha=0.5)\n",
    "ax[1].set_title('x11 mean distribution (99th percentile)')\n",
    "ax[1].set_xlabel('x11 mean value')\n",
    "ax[1].set_ylabel('Instance count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 6, figsize=(30, 5))\n",
    "\n",
    "for i in range(6):\n",
    "    x_mean = mean_traits.iloc[:, i]\n",
    "    x_mean = x_mean[x_mean.between(x_mean.quantile(0), x_mean.quantile(0.99))]\n",
    "    x_mean = x_mean.drop_duplicates()\n",
    "    \n",
    "    ax[i].hist(x_mean, bins=100, log=True, alpha=0.5, label=f'Normalized x{i} mean')\n",
    "    ax[i].set_title(f'x{i} mean distribution (99th percentile)')\n",
    "    ax[i].set_xlabel(f'x{i} mean value')\n",
    "    ax[i].set_ylabel('Log count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us some idea of the distribution of the targets we are training for. One thing to consider is that this distribution of traits might not be representative of the general traits of plant species. \n",
    "*Notes*\n",
    "- we can maybe check some location data to see where most of the images came from to see if certain traits indicate a geographic bias "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sd traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_traits = df.iloc[:, 170:176]\n",
    "display(sd_traits.head(2))\n",
    "def sd_normalizer(col):\n",
    "    sd = sd_traits.iloc[:, col]\n",
    "    return (sd - sd.min()) / (sd.max() - sd.min())\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(6):  \n",
    "    plt.hist(sd_normalizer(i), log=True, bins=100, alpha=0.5, label=f'Normalized SD x{i}')\n",
    "\n",
    "plt.title('Normalized standard deviation trait distributions')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might be an even better representative that we are training towards a biased sample. Intuitively I'd assume that traits would probably have a large variance if we have a geographic diversity in our sample, but here it seems this is less so the case for most of the deviations maybe ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ancillary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancillary_data = df.iloc[:, 1:164]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "# worldclim \n",
    "worldclim = ancillary_data.filter(like='WORLDCLIM')\n",
    "\n",
    "g = sns.pairplot(worldclim, diag_kind='kde', plot_kws={'alpha': 0.5})\n",
    "\n",
    "# remove labels \n",
    "for ax in g.axes.flatten():\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xlabel('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worldclim = ancillary_data.filter(like='WORLDCLIM')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "corr = worldclim.corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', cbar=True)\n",
    "ax.set_title('Worldclim feature correlation heatmap')\n",
    "\n",
    "high_corr = [i for i in corr[corr > 0.9].stack().index.tolist() if i[0] != i[1]]\n",
    "high_corr = list(set([tuple(sorted(i)) for i in high_corr]))\n",
    "\n",
    "scaled_worldclim_data = (worldclim - worldclim.min()) / (worldclim.max() - worldclim.min())\n",
    "print(high_corr)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "pca_result = pca.fit_transform(scaled_worldclim_data)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Plot PCA\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.scatter(pca_result[:, 0], pca_result[:, 1], pca_result[:, 2], c=pca_result[:, 0], cmap='viridis', s=10)\n",
    "ax1.set_title('PCA of worldclim features')\n",
    "\n",
    "reconstructed_pca = pca.inverse_transform(pca_result)\n",
    "loss_pca = np.mean((scaled_worldclim_data - reconstructed_pca) ** 2)\n",
    "\n",
    "print(f'PCA Reconstruction Loss: {loss_pca}')\n",
    "\n",
    "# UMAP\n",
    "reducer = umap.UMAP(n_components=3)\n",
    "embedding = reducer.fit_transform(scaled_worldclim_data)\n",
    "\n",
    "# Plot UMAP\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "ax2.scatter(embedding[:, 0], embedding[:, 1], embedding[:, 2], c=embedding[:, 0], cmap='viridis', s=10)\n",
    "ax2.set_title('UMAP embedding of worldclim features')\n",
    "\n",
    "reconstructed_umap = reducer.inverse_transform(embedding)\n",
    "loss_umap = np.mean((scaled_worldclim_data - reconstructed_umap) ** 2)\n",
    "\n",
    "print(f'UMAP Reconstruction Loss: {loss_umap}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we start of by just looking at worldclim subset of our training data and testing out some of the basic methods we are going to then use on the broader dataset, namely finding correlations and then comparing the performance of PCA vs UAMP in the dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancillary_data = df.iloc[:, 1:164]\n",
    " \n",
    "# select allcolumns with a name containing WORLDCLIM\n",
    "worldclim   = ancillary_data.filter(like='WORLDCLIM')\n",
    "soil        = ancillary_data.filter(like='SOIL')\n",
    "modis       = ancillary_data.filter(like='MODIS')\n",
    "vod         = ancillary_data.filter(like='VOD')\n",
    "\n",
    "print(\n",
    "    f\"lengths {len(worldclim.columns)}, {len(soil.columns)}, {len(modis.columns)}, {len(vod.columns)}\"\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 x 1 subplot of correlation heatmaps \n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# soil \n",
    "\n",
    "soil_corr = soil.corr()\n",
    "sns.heatmap(soil_corr, ax=ax[0], cmap='coolwarm', annot=False)\n",
    "\n",
    "ax[0].set_title('Soil feature correlation heatmap')\n",
    "ax[0].set_xticklabels([])\n",
    "ax[0].set_yticklabels([])\n",
    "\n",
    "# modis\n",
    "\n",
    "modis_corr = modis.corr()\n",
    "sns.heatmap(modis_corr, ax=ax[1], cmap='coolwarm', annot=False)\n",
    "\n",
    "ax[1].set_title('MODIS feature correlation heatmap')\n",
    "ax[1].set_xticklabels([])\n",
    "ax[1].set_yticklabels([])\n",
    "\n",
    "# vod\n",
    "\n",
    "vod_corr = vod.corr()\n",
    "sns.heatmap(vod_corr, ax=ax[2], cmap='coolwarm', annot=False)\n",
    "\n",
    "ax[2].set_title('VOD feature correlation heatmap')\n",
    "ax[2].set_xticklabels([])\n",
    "ax[2].set_yticklabels([])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = [worldclim.corr(), soil.corr(), modis.corr(), vod.corr()]\n",
    "\n",
    "high_corr = [] \n",
    "\n",
    "for corr in correlations:\n",
    "    high_corr += [\n",
    "        i for i in corr[corr > 0.9].stack().index.tolist() if i[0] != i[1]\n",
    "    ]\n",
    "    \n",
    "mod_corr = []\n",
    "\n",
    "for corr in correlations:\n",
    "    mod_corr += [\n",
    "        i for i in corr[corr > 0.65].stack().index.tolist() if i[0] != i[1]\n",
    "    ]\n",
    "    \n",
    "total_cols = 164 \n",
    "all_comb_num = total_cols * (total_cols - 1) / 2\n",
    "    \n",
    "high_corr = list(set([tuple(sorted(i)) for i in high_corr]))\n",
    "print(\n",
    "    f'num combinations with high correlation: {len(high_corr)}' + \n",
    "    f' ({len(high_corr) / all_comb_num * 100:.2f}%)' +\n",
    "    f' tot = {all_comb_num}' \n",
    ")\n",
    "print(\n",
    "    f'num combinations with moderate correlation: {len(mod_corr)}' +\n",
    "    f' ({len(mod_corr) / all_comb_num * 100:.2f}%)' +\n",
    "    f' tot = {all_comb_num}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "\n",
    "scaler = StandardScaler()\n",
    "ancillary_data = df.iloc[:, 1:164]\n",
    "\n",
    "# random sample of ancillary data subset \n",
    "ancillary_data = scaler.fit_transform(ancillary_data)\n",
    "\n",
    "# PCA \n",
    "pca = PCA(n_components=0.95)\n",
    "pca_result = pca.fit_transform(ancillary_data)\n",
    "\n",
    "# UMAP\n",
    "reducer = umap.UMAP(n_components=30)\n",
    "embedding = reducer.fit_transform(ancillary_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter over all rows in df and look for missing data\n",
    "\n",
    "bad_rows = []\n",
    "incomplete_cols = [n for n in config.aux_class_names if n.startswith(\"X\") and n.endswith(\"_sd\")]  # there is some missing data in the X*_sd columns\n",
    "for i, row in df.iterrows():\n",
    "    # check if any of those is a 0, -1 or 1\n",
    "    if row[incomplete_cols].isin([0, -1, 1]).any():\n",
    "        bad_rows.append(i)\n",
    "        # replace them with nans instead\n",
    "        df.loc[i, incomplete_cols] = np.nan\n",
    "\n",
    "# remove duplicates\n",
    "bad_rows = set(bad_rows)\n",
    "percentage_bad = len(bad_rows) / len(df) * 100\n",
    "print(f\"Found {len(bad_rows)} bad rows ({len(bad_rows)}/{len(df)}) ({percentage_bad:.2f}%)\")\n",
    "\n",
    "# display top 5 bad rows\n",
    "missing_data_df = df.iloc[list(bad_rows)]\n",
    "missing_data_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### where ?\n",
    "\n",
    "Features marked as `X*_sd` are missing for almost 30% of the data (16387/55489) (29.53%). The rest of the data seems to be complete, with no other missing/zeroed values. We can ignore these rows, or we can simply ignore those features as it is a significant amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imputation of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use KNN to try to fill the X*_sd columns\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5, missing_values=-1, weights='distance', metric='nan_euclidean')\n",
    "\n",
    "tabular = df.iloc[:, 170:176]\n",
    "imputed_tabluar = imputer.fit_transform(tabular)\n",
    "imputed_df = pd.DataFrame(imputed_tabluar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>WORLDCLIM_BIO1_annual_mean_temperature</th>\n",
       "      <th>WORLDCLIM_BIO12_annual_precipitation</th>\n",
       "      <th>WORLDCLIM_BIO13.BIO14_delta_precipitation_of_wettest_and_dryest_month</th>\n",
       "      <th>WORLDCLIM_BIO15_precipitation_seasonality</th>\n",
       "      <th>WORLDCLIM_BIO4_temperature_seasonality</th>\n",
       "      <th>WORLDCLIM_BIO7_temperature_annual_range</th>\n",
       "      <th>SOIL_bdod_0.5cm_mean_0.01_deg</th>\n",
       "      <th>SOIL_bdod_100.200cm_mean_0.01_deg</th>\n",
       "      <th>SOIL_bdod_15.30cm_mean_0.01_deg</th>\n",
       "      <th>...</th>\n",
       "      <th>X26_mean</th>\n",
       "      <th>X50_mean</th>\n",
       "      <th>X3112_mean</th>\n",
       "      <th>X4_sd_sd</th>\n",
       "      <th>X11_sd_sd</th>\n",
       "      <th>X18_sd_sd</th>\n",
       "      <th>X26_sd_sd</th>\n",
       "      <th>X50_sd_sd</th>\n",
       "      <th>X3112_sd_sd</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>192027691</td>\n",
       "      <td>12.235703</td>\n",
       "      <td>374.466675</td>\n",
       "      <td>62.524445</td>\n",
       "      <td>72.256844</td>\n",
       "      <td>773.592041</td>\n",
       "      <td>33.277779</td>\n",
       "      <td>125</td>\n",
       "      <td>149</td>\n",
       "      <td>136</td>\n",
       "      <td>...</td>\n",
       "      <td>1.243779</td>\n",
       "      <td>1.849375</td>\n",
       "      <td>50.216034</td>\n",
       "      <td>0.008921</td>\n",
       "      <td>1.601473</td>\n",
       "      <td>0.025441</td>\n",
       "      <td>0.153608</td>\n",
       "      <td>0.279610</td>\n",
       "      <td>15.045054</td>\n",
       "      <td>../data/train_images/192027691.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>195542235</td>\n",
       "      <td>17.270555</td>\n",
       "      <td>90.239998</td>\n",
       "      <td>10.351111</td>\n",
       "      <td>38.220940</td>\n",
       "      <td>859.193298</td>\n",
       "      <td>40.009777</td>\n",
       "      <td>124</td>\n",
       "      <td>144</td>\n",
       "      <td>138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.642940</td>\n",
       "      <td>1.353468</td>\n",
       "      <td>574.098472</td>\n",
       "      <td>0.003102</td>\n",
       "      <td>0.258078</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>0.034630</td>\n",
       "      <td>0.010165</td>\n",
       "      <td>11.004477</td>\n",
       "      <td>../data/train_images/195542235.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>196639184</td>\n",
       "      <td>14.254504</td>\n",
       "      <td>902.071411</td>\n",
       "      <td>49.642857</td>\n",
       "      <td>17.873655</td>\n",
       "      <td>387.977753</td>\n",
       "      <td>22.807142</td>\n",
       "      <td>107</td>\n",
       "      <td>133</td>\n",
       "      <td>119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.395241</td>\n",
       "      <td>2.343153</td>\n",
       "      <td>1130.096731</td>\n",
       "      <td>0.007833</td>\n",
       "      <td>1.073573</td>\n",
       "      <td>16.003476</td>\n",
       "      <td>110.733150</td>\n",
       "      <td>0.075108</td>\n",
       "      <td>453.017146</td>\n",
       "      <td>../data/train_images/196639184.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>195728812</td>\n",
       "      <td>18.680834</td>\n",
       "      <td>1473.933350</td>\n",
       "      <td>163.100006</td>\n",
       "      <td>45.009758</td>\n",
       "      <td>381.053986</td>\n",
       "      <td>20.436666</td>\n",
       "      <td>120</td>\n",
       "      <td>131</td>\n",
       "      <td>125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154200</td>\n",
       "      <td>1.155308</td>\n",
       "      <td>1042.686546</td>\n",
       "      <td>0.011692</td>\n",
       "      <td>2.818356</td>\n",
       "      <td>0.110673</td>\n",
       "      <td>0.011334</td>\n",
       "      <td>0.229224</td>\n",
       "      <td>141.857187</td>\n",
       "      <td>../data/train_images/195728812.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>195251545</td>\n",
       "      <td>0.673204</td>\n",
       "      <td>530.088867</td>\n",
       "      <td>50.857777</td>\n",
       "      <td>38.230709</td>\n",
       "      <td>1323.526855</td>\n",
       "      <td>45.891998</td>\n",
       "      <td>91</td>\n",
       "      <td>146</td>\n",
       "      <td>120</td>\n",
       "      <td>...</td>\n",
       "      <td>10.919966</td>\n",
       "      <td>2.246226</td>\n",
       "      <td>2386.467180</td>\n",
       "      <td>0.006157</td>\n",
       "      <td>1.128000</td>\n",
       "      <td>0.026996</td>\n",
       "      <td>0.553815</td>\n",
       "      <td>0.107092</td>\n",
       "      <td>87.146899</td>\n",
       "      <td>../data/train_images/195251545.jpeg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 177 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  WORLDCLIM_BIO1_annual_mean_temperature  \\\n",
       "0  192027691                               12.235703   \n",
       "1  195542235                               17.270555   \n",
       "2  196639184                               14.254504   \n",
       "3  195728812                               18.680834   \n",
       "4  195251545                                0.673204   \n",
       "\n",
       "   WORLDCLIM_BIO12_annual_precipitation  \\\n",
       "0                            374.466675   \n",
       "1                             90.239998   \n",
       "2                            902.071411   \n",
       "3                           1473.933350   \n",
       "4                            530.088867   \n",
       "\n",
       "   WORLDCLIM_BIO13.BIO14_delta_precipitation_of_wettest_and_dryest_month  \\\n",
       "0                                          62.524445                       \n",
       "1                                          10.351111                       \n",
       "2                                          49.642857                       \n",
       "3                                         163.100006                       \n",
       "4                                          50.857777                       \n",
       "\n",
       "   WORLDCLIM_BIO15_precipitation_seasonality  \\\n",
       "0                                  72.256844   \n",
       "1                                  38.220940   \n",
       "2                                  17.873655   \n",
       "3                                  45.009758   \n",
       "4                                  38.230709   \n",
       "\n",
       "   WORLDCLIM_BIO4_temperature_seasonality  \\\n",
       "0                              773.592041   \n",
       "1                              859.193298   \n",
       "2                              387.977753   \n",
       "3                              381.053986   \n",
       "4                             1323.526855   \n",
       "\n",
       "   WORLDCLIM_BIO7_temperature_annual_range  SOIL_bdod_0.5cm_mean_0.01_deg  \\\n",
       "0                                33.277779                            125   \n",
       "1                                40.009777                            124   \n",
       "2                                22.807142                            107   \n",
       "3                                20.436666                            120   \n",
       "4                                45.891998                             91   \n",
       "\n",
       "   SOIL_bdod_100.200cm_mean_0.01_deg  SOIL_bdod_15.30cm_mean_0.01_deg  ...  \\\n",
       "0                                149                              136  ...   \n",
       "1                                144                              138  ...   \n",
       "2                                133                              119  ...   \n",
       "3                                131                              125  ...   \n",
       "4                                146                              120  ...   \n",
       "\n",
       "    X26_mean  X50_mean   X3112_mean  X4_sd_sd  X11_sd_sd  X18_sd_sd  \\\n",
       "0   1.243779  1.849375    50.216034  0.008921   1.601473   0.025441   \n",
       "1   0.642940  1.353468   574.098472  0.003102   0.258078   0.000866   \n",
       "2   0.395241  2.343153  1130.096731  0.007833   1.073573  16.003476   \n",
       "3   0.154200  1.155308  1042.686546  0.011692   2.818356   0.110673   \n",
       "4  10.919966  2.246226  2386.467180  0.006157   1.128000   0.026996   \n",
       "\n",
       "    X26_sd_sd  X50_sd_sd  X3112_sd_sd                           image_path  \n",
       "0    0.153608   0.279610    15.045054  ../data/train_images/192027691.jpeg  \n",
       "1    0.034630   0.010165    11.004477  ../data/train_images/195542235.jpeg  \n",
       "2  110.733150   0.075108   453.017146  ../data/train_images/196639184.jpeg  \n",
       "3    0.011334   0.229224   141.857187  ../data/train_images/195728812.jpeg  \n",
       "4    0.553815   0.107092    87.146899  ../data/train_images/195251545.jpeg  \n",
       "\n",
       "[5 rows x 177 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imputed_df = imputed_df.rename(columns={i: f\"{config.aux_class_names[i]}_sd\" for i in range(6)})\n",
    "df = pd.concat([df.iloc[:, :170], imputed_df, df['image_path']], axis=1)\n",
    "\n",
    "# save the imputed data to a new csv in 'data/test_knn_imputed.csv'\n",
    "df.to_csv(DATA_PATH + '/train_knn_imputed.csv', index=False)\n",
    "\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice the df to only include the columns we want\n",
    "img_df = df[[\"image_path\"] + config.class_names]\n",
    "# sample 5 rows \n",
    "img_df = img_df.sample(5)\n",
    "\n",
    "img, traits = list(img_df[\"image_path\"].values), list(img_df[config.class_names].values)\n",
    "\n",
    "num_imgs, num_cols = 5, 5\n",
    "\n",
    "plt.figure(figsize=(5 * num_cols, num_imgs // num_cols * 5))\n",
    "for i, (img, traits) in enumerate(zip(img, traits)):\n",
    "    plt.subplot(num_imgs // num_cols, num_cols, i + 1)\n",
    "    img = cv2.imread(img)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    formatted_tar = \"\\n\".join(\n",
    "        [\n",
    "            \", \".join(\n",
    "                f\"{name.replace('_mean','')}: {val:.2f}\"\n",
    "                for name, val in zip(config.class_names[j : j + 3], traits[j : j + 3])\n",
    "            )\n",
    "            for j in range(0, len(config.class_names), 3)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"[{formatted_tar}]\")\n",
    "    plt.axis(\"off\")\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possibly some augmenation / feature enhancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function for model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outline for the image model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backbone comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing differen backbones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation + tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluting model and tuning hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outline for the ensemble model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation + tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating ensemble model and tuning hyperparameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
